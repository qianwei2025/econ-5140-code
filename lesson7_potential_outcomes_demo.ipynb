{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECON 5140: Applied Econometrics\n",
    "## Lesson 7: Potential Outcomes Framework and Causal Inference\n",
    "\n",
    "This notebook demonstrates key concepts from Lesson 7 using concrete numerical examples and simulations.\n",
    "\n",
    "**Topics covered:**\n",
    "1. Potential outcomes: ATE, ATT, ATU\n",
    "2. Simple difference in outcomes (SDO) vs causal effects\n",
    "3. Selection bias and the bias decomposition\n",
    "4. Random assignment: when SDO = ATE\n",
    "5. Confounding and regression: when OLS fails and when it works\n",
    "6. DAGs: visualizing confounding and colliders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The 10-Patient Example (from Lecture)\n",
    "\n",
    "Ten cancer patients, two treatments:\n",
    "- **D = 1**: Surgery\n",
    "- **D = 0**: Chemotherapy\n",
    "\n",
    "Patients are assigned to treatment with \"perfect wisdom\" (they choose the treatment that gives them the better outcome). This creates selection: treatment is *not* random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the lecture slides (10 patients)\n",
    "df = pd.DataFrame({\n",
    "    'patient': range(1, 11),\n",
    "    'Y1': [7, 5, 5, 7, 4, 10, 1, 5, 3, 9],   # Outcome under surgery\n",
    "    'Y0': [1, 6, 1, 8, 2, 1, 10, 6, 7, 8],   # Outcome under chemo\n",
    "})\n",
    "df['delta'] = df['Y1'] - df['Y0']  # Individual treatment effect\n",
    "\n",
    "# Assignment with \"perfect wisdom\": choose treatment that gives better outcome\n",
    "df['D'] = (df['Y1'] > df['Y0']).astype(int)\n",
    "\n",
    "# Observed outcome: Y = D*Y1 + (1-D)*Y0\n",
    "df['Y_obs'] = df['D'] * df['Y1'] + (1 - df['D']) * df['Y0']\n",
    "\n",
    "print(\"10-Patient Data:\")\n",
    "print(df.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Causal estimands (we can compute these because we have BOTH potential outcomes) ---\n",
    "ATE = (df['Y1'] - df['Y0']).mean()\n",
    "ATT = df.loc[df['D'] == 1, 'delta'].mean()  # Among those who got surgery\n",
    "ATU = df.loc[df['D'] == 0, 'delta'].mean()  # Among those who got chemo\n",
    "\n",
    "p = df['D'].mean()  # Pr(D=1)\n",
    "print(\"Causal estimands (from potential outcomes):\")\n",
    "print(f\"  ATE = E[Y(1) - Y(0)] = {ATE:.2f}\")\n",
    "print(f\"  ATT = E[Y(1) - Y(0) | D=1] = {ATT:.2f}\")\n",
    "print(f\"  ATU = E[Y(1) - Y(0) | D=0] = {ATU:.2f}\")\n",
    "print(f\"  ATE = p*ATT + (1-p)*ATU = {p:.1f}*{ATT:.2f} + {1-p:.1f}*{ATU:.2f} = {p*ATT + (1-p)*ATU:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple Difference in Outcomes (what we observe in practice) ---\n",
    "mean_Y_treated = df.loc[df['D'] == 1, 'Y_obs'].mean()\n",
    "mean_Y_control = df.loc[df['D'] == 0, 'Y_obs'].mean()\n",
    "SDO = mean_Y_treated - mean_Y_control\n",
    "\n",
    "print(\"Simple Difference in Outcomes (naive comparison):\")\n",
    "print(f\"  E[Y|D=1] = {mean_Y_treated:.2f}\")\n",
    "print(f\"  E[Y|D=0] = {mean_Y_control:.2f}\")\n",
    "print(f\"  SDO = E[Y|D=1] - E[Y|D=0] = {SDO:.2f}\")\n",
    "print()\n",
    "print(\"SDO ≠ ATE! The naive comparison is biased because of selection.\")\n",
    "print(f\"Bias = SDO - ATE = {SDO - ATE:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bias decomposition (from lecture) ---\n",
    "# SDO = ATE + Selection bias + HTE bias\n",
    "\n",
    "E_Y0_D1 = df.loc[df['D'] == 1, 'Y0'].mean()  # Avg Y(0) among treated\n",
    "E_Y0_D0 = df.loc[df['D'] == 0, 'Y0'].mean()  # Avg Y(0) among control\n",
    "selection_bias = E_Y0_D1 - E_Y0_D0\n",
    "hte_bias = p * (ATT - ATU)\n",
    "\n",
    "print(\"Bias decomposition:\")\n",
    "print(f\"  1. ATE = {ATE:.2f}\")\n",
    "print(f\"  2. Selection bias = E[Y(0)|D=1] - E[Y(0)|D=0] = {E_Y0_D1:.2f} - {E_Y0_D0:.2f} = {selection_bias:.2f}\")\n",
    "print(f\"     (Treated would have done worse under chemo than control did)\")\n",
    "print(f\"  3. HTE bias = p*(ATT - ATU) = {p:.2f}*({ATT:.2f} - ({ATU:.2f})) = {hte_bias:.2f}\")\n",
    "print()\n",
    "print(f\"  SDO = ATE + Selection bias + HTE bias\")\n",
    "print(f\"  SDO = {ATE:.2f} + ({selection_bias:.2f}) + {hte_bias:.2f} = {ATE + selection_bias + hte_bias:.2f}\")\n",
    "print()\n",
    "print(\"Interpretation: Unless selection bias = 0, the difference in means is not causal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Random Assignment — When SDO = ATE\n",
    "\n",
    "If treatment were **randomly assigned**, then (Y(1), Y(0)) ⊥ D. In that case:\n",
    "- E[Y(0)|D=1] = E[Y(0)|D=0] → selection bias = 0\n",
    "- SDO = ATE\n",
    "\n",
    "We simulate the same 10 patients but now assign D randomly (coin flip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same potential outcomes, but RANDOM assignment\n",
    "np.random.seed(123)\n",
    "df_rand = df[['patient', 'Y1', 'Y0', 'delta']].copy()\n",
    "df_rand['D'] = np.random.binomial(1, 0.5, size=10)  # Random 50-50 assignment\n",
    "df_rand['Y_obs'] = df_rand['D'] * df_rand['Y1'] + (1 - df_rand['D']) * df_rand['Y0']\n",
    "\n",
    "SDO_rand = df_rand.loc[df_rand['D'] == 1, 'Y_obs'].mean() - df_rand.loc[df_rand['D'] == 0, 'Y_obs'].mean()\n",
    "\n",
    "print(\"With RANDOM assignment:\")\n",
    "print(f\"  ATE (unchanged) = {ATE:.2f}\")\n",
    "print(f\"  SDO = {SDO_rand:.2f}\")\n",
    "print(f\"  SDO ≈ ATE? {abs(SDO_rand - ATE) < 1.0}\")\n",
    "print()\n",
    "print(\"With only 10 units, SDO is noisy but unbiased. With many units, SDO → ATE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate many random assignments to show SDO converges to ATE\n",
    "n_sim = 5000\n",
    "sdo_list = []\n",
    "for _ in range(n_sim):\n",
    "    D_sim = np.random.binomial(1, 0.5, size=10)\n",
    "    Y_obs_sim = D_sim * df['Y1'].values + (1 - D_sim) * df['Y0'].values\n",
    "    sdo = Y_obs_sim[D_sim == 1].mean() - Y_obs_sim[D_sim == 0].mean()\n",
    "    sdo_list.append(sdo)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(sdo_list, bins=50, edgecolor='white', alpha=0.8)\n",
    "plt.axvline(ATE, color='red', linestyle='--', linewidth=2, label=f'True ATE = {ATE}')\n",
    "plt.axvline(np.mean(sdo_list), color='green', linestyle='-', linewidth=1.5, label=f'Mean SDO = {np.mean(sdo_list):.2f}')\n",
    "plt.xlabel('SDO (Simple Difference in Outcomes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of SDO Under Random Assignment (5000 simulations)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Mean of SDO across simulations: {np.mean(sdo_list):.2f} (should equal ATE = {ATE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Confounding and Regression\n",
    "\n",
    "**Setup:** We want to estimate the effect of training (D) on earnings (Y). But ability (X) affects both:\n",
    "- Higher ability → more likely to get training\n",
    "- Higher ability → higher earnings\n",
    "\n",
    "X is a **confounder**. The DAG: D ← X → Y (and D → Y).\n",
    "\n",
    "**Naive regression** Y = α + βD + ε: β captures association, not causation.\n",
    "\n",
    "**Correct regression** Y = α + βD + γX + ε: controlling for X blocks the backdoor path; β estimates the causal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data with confounding\n",
    "# True DGP: Y = 50 + 10*D + 5*X + noise (causal effect of D = 10)\n",
    "# D is influenced by X: higher X → more likely D=1\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "X = np.random.normal(0, 1, n)  # Ability (standardized)\n",
    "D = (np.random.uniform(0, 1, n) < 0.3 + 0.4 * (X > 0)).astype(int)  # X increases Pr(D=1)\n",
    "Y = 50 + 10*D + 5*X + np.random.normal(0, 5, n)  # True causal effect of D = 10\n",
    "\n",
    "df_conf = pd.DataFrame({'Y': Y, 'D': D, 'X': X})\n",
    "\n",
    "# Naive regression: Y ~ D (omit X)\n",
    "X_naive = sm.add_constant(df_conf['D'])\n",
    "model_naive = OLS(df_conf['Y'], X_naive).fit()\n",
    "\n",
    "# Correct regression: Y ~ D + X\n",
    "X_correct = sm.add_constant(df_conf[['D', 'X']])\n",
    "model_correct = OLS(df_conf['Y'], X_correct).fit()\n",
    "\n",
    "print(\"Naive regression (Y ~ D): omitting confounder X\")\n",
    "print(model_naive.summary().tables[1])\n",
    "print()\n",
    "print(\"Correct regression (Y ~ D + X): controlling for confounder\")\n",
    "print(model_correct.summary().tables[1])\n",
    "print()\n",
    "print(\"True causal effect of D: 10\")\n",
    "print(f\"Naive β (biased): {model_naive.params['D']:.2f}\")\n",
    "print(f\"Correct β (unbiased): {model_correct.params['D']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: DAGs — Visualizing Confounding and Colliders\n",
    "\n",
    "DAGs encode causal assumptions. We draw:\n",
    "1. **Confounding**: D ← X → Y and D → Y. X is a confounder; we must control for it.\n",
    "2. **Collider**: D → X ← Y. X is a collider; conditioning on X *creates* bias (opens a backdoor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dag(edges, title, ax):\n",
    "    \"\"\"Draw a simple DAG. edges: list of (from, to) tuples.\"\"\"\n",
    "    pos = {'D': (0.5, 0.5), 'Y': (1, 0.5), 'X': (0.75, 0.9)}\n",
    "    for i, (a, b) in enumerate(edges):\n",
    "        if a not in pos:\n",
    "            pos[a] = (0.2 + 0.3*i, 0.2)\n",
    "        if b not in pos:\n",
    "            pos[b] = (0.8 - 0.3*i, 0.8)\n",
    "    \n",
    "    # Adjust for common DAGs\n",
    "    if ('X', 'D') in edges or ('D', 'X') in edges:\n",
    "        pos = {'D': (0.3, 0.5), 'Y': (0.9, 0.5), 'X': (0.6, 0.9)}\n",
    "    if ('D', 'X') in edges and ('Y', 'X') in edges:  # Collider\n",
    "        pos = {'D': (0.2, 0.5), 'Y': (0.8, 0.5), 'X': (0.5, 0.9)}\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    for (a, b) in edges:\n",
    "        x1, y1 = pos[a]\n",
    "        x2, y2 = pos[b]\n",
    "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                    arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "    \n",
    "    for node, (x, y) in pos.items():\n",
    "        ax.plot(x, y, 'o', markersize=20, color='steelblue', markeredgecolor='black', markeredgewidth=1.5)\n",
    "        ax.text(x, y, node, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Confounding: D <- X -> Y, D -> Y\n",
    "draw_dag([('X', 'D'), ('X', 'Y'), ('D', 'Y')], 'Confounding: D ← X → Y, D → Y\\n(Control for X to identify causal effect)', axes[0])\n",
    "\n",
    "# Collider: D -> X <- Y\n",
    "draw_dag([('D', 'X'), ('Y', 'X')], 'Collider: D → X ← Y\\n(Do NOT condition on X — creates bias)', axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate collider bias with simulation\n",
    "# D and Y are independent in population. But both cause X (collider).\n",
    "# If we condition on X (e.g., restrict to X=1), D and Y become associated.\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 5000\n",
    "D = np.random.binomial(1, 0.5, n)  # Random\n",
    "Y = np.random.normal(0, 1, n)      # Independent of D\n",
    "X = D + Y + np.random.normal(0, 0.5, n)  # Collider: both D and Y cause X\n",
    "\n",
    "print(\"Collider bias demonstration:\")\n",
    "print(f\"  In full sample: Corr(D, Y) = {np.corrcoef(D, Y)[0,1]:.3f} (should be ~0)\")\n",
    "\n",
    "# Condition on X being high (top 25%)\n",
    "high_X = X > np.percentile(X, 75)\n",
    "D_cond = D[high_X]\n",
    "Y_cond = Y[high_X]\n",
    "print(f\"  Conditioning on X high: Corr(D, Y) = {np.corrcoef(D_cond, Y_cond)[0,1]:.3f}\")\n",
    "print()\n",
    "print(\"  Interpretation: Among those with high X, D and Y are negatively associated.\")\n",
    "print(\"  (High D + low Y can give high X; low D + high Y can also give high X.)\")\n",
    "print(\"  Conditioning on the collider creates spurious correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **Potential outcomes** define causal effects (ATE, ATT, ATU) but are never fully observed.\n",
    "2. **SDO = E[Y|D=1] - E[Y|D=0]** equals ATE only under random assignment. With selection, SDO is biased.\n",
    "3. **Random assignment** eliminates selection bias; RCTs are the gold standard.\n",
    "4. **Confounders** create backdoor paths; we must control for them to identify causal effects.\n",
    "5. **Colliders** are different: conditioning on them *creates* bias. Do not control for colliders.\n",
    "6. **Regression** estimates causal effects only when identification assumptions hold. Interpretation is a causal decision, not a statistical reflex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
